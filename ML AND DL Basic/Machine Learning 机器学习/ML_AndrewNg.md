#### 1 Supervised Learning 有监督学习

##### 1.1 Regression 线性回归、成本函数、梯度下降

1.单变量线性回归：
$$
f_(w, b) = w*x + b\tag1
$$
2.成本函数：Cost function,衡量模型拟合程度,平方误差成本函数
$$
J(w, b) = \frac{1}{2*m}*\sum_{i=1}^m(y^--y)^2\tag2
$$
其中$y^-$为预测值，$y$为实际值。期待寻找$(w,b) => minJ(w,b)$。存在求和过程，每一个$J(w,b)$为$(w,b)$的函数，求和包含了所有的$x$。

==Target:寻找$(w,b)$，使得$J$最小，即差异最低==

==损失函数(Loss)衡量单个样本的预测误差，代价函数(Cost)是所有样本损失的平均。==

3.梯度下降：逐步更新，同步更新$(w, b)$，得到极小值不一定是全局最小值。线性函数总是极小值就为最小值
$$
w = w-α\frac{∂J(w,b)}{∂w}\tag3
$$
其中$α$为学习率，应适当选择，过小学习太慢，过大无法收敛



##### 1.2 多变量线性回归、特征工程、scikit-learn

1.多特征重要点：向量化,`np.array()加快计算速度，并行处理`

```python
# np.array表现形式没有逗号分隔
# zeros初始化的是浮点数
arr1 = np.zeros(4) = np.zeros((4,)) = [0. 0. 0. 0.]
arr2 = np.arange(4.)
arr3 = np.random.rand(4)
arr4 = np.array[1, 2, 3, 4]
# NumPy同样支持索引与切片
# NumPy的核心是向量，每一个数组都是向量组成，支持并行计算
arr1 = arr1 + arr2				# 对应元素相加
arr3 = arr2 * arr3				# 点积，得到一个数值
arr4 = arr4 * 2					# 数乘

arr = np.zeros((1, 5))			# (维度，数目) == (行，列)
arr = np.zeros((2, 1))

# reshape生成矩阵 -1代表自动计算
np.arange(6).reshape(3, 2) == np.arange(6).reshape(-1, 2)
```

2.特征缩放：加快梯度下降速度，避免某个特征数值过大，对整体函数的影响也过大。==Z-scored归一化==，对每个特征，即矩阵中的每一列单独做。
$$
x_j^{(i)} = \frac{x_j^{(i)} - μ_j}{σ_j}\tag4
$$
其中$μ_j = \frac{1}{m}∑_{i=0}^{m-1}x_j^{(i)}$为平均值，$σ_j^2 = \frac{1}{m}\sum_{i=0}^{m-1}(x_j^{(i)}-μ_j)^2$为标准差

```python
mu = np.mean(X, axis = 0)
sigme = np.std(X, axis = 0)
X_norm = (X - mu) / sigma
# axis = 0 代表每次取一列，沿着行的方向前进
# 则 axis = 1代表每次取一行
```

3.特征工程：选取适当的特征，适当的元素、元素的组合等。一切可能令机器更好的学习到目标。



##### 1.3 Logistic回归，过拟合，正则化

1.激活函数，Sigmoid：逻辑回归用于分类，借助于激活函数.Sigmoid激活函数，将[0, 0.5]映射为复数，而[0.5, 1]映射为正数，凭借正负二分类。解释为概率，为某个分类的概率更大，可量化
$$
g(f(x)) = \frac{1}{1+e^{-(wx+b)}}\tag5
$$
2.决策边界：分类之间的边界，也就是$f(x)$本身的图象

3.损失函数：显然，平方差损失函数仅适用于极小值就等于最小值的情况。在复杂曲线中，不再适用。以下是损失函数。
$$
J(w,b) = \frac{1}{2m}\sum_{i=1}^mL(g(Z))\tag6
$$

$$
L(g(Z)) = \left
\{
\begin{matrix}
-log(g(Z)) \ \ \ \ \ \ \ if \ \ y = 1 \\
-log(1-g(Z))\ \ \ \ \ \ \ if \ \ y = 0
\end{matrix} 
\right.
$$

解释：$L()$中为加上激活函数后的线性回归，则$∈[0, 1]$，因此，当内部越接近1，则cost->0

4.==正则化：对抗过拟合的方法之一==，在损失函数中对某些自变量设置相当大的系数，因此为了和原损失函数得到相同的结果，会让对应的$w$相当小，也就变相的降低了多维度中某些维度的作用。
$$
J(w, b) = XXX + 1000w_3^2 + 1000w_4^2\tag7
$$
在工程中，并不能显式的预先了解哪些特征应该被”惩罚“，因此，对所有特征做==统一正则化==。
$$
J(w,b) = XXX + \frac{λ}{2m}\sum_{j=0}^{n-1}w_j^2\tag8
$$
同样的，梯度函数也要加上正则化项。



##### 1.4 神经网络入门

1.输入层、输出层：输入层的某一个神经元可由不同的输入源组合对应影响，比如$x_1*x_2$。输出层输出的是==预测概率==。

- 在实践中，为了考虑全面不失泛化，输入层到每个节点应该改全部获取
- <img src="D:\Study\CS Study\Note 2024-\All Photo\image-20251012112333108.png" alt="image-20251012112333108" style="zoom:80%;" />
- 因此，神经网络不需要手动设计特征，其会自动学习哪些特征组合可能更好。使得学习变得更容易。输入不同的特征，神经网络会==自动学习“边缘线条->组成部件->构成模糊整体”==，这些内容是神经网络固定的。根据不同的输入，学习不同的部分。
- 需要设计：隐藏层的数目、每个隐藏层神经元的数目

2.神经网络层：第一隐藏层接受所有特征组成的向量，但是==每个神经元最终输出一个标量==。因为$f(x) = \vec{w}*x+b$是点积。

- 输入层也称第0层，4层神经网络 == 隐藏 + 输出

例如：存在6个特征，3层神经网络，每层神经网络的神经元数是3、3、1。则第一层隐藏层接受6个6元素向量，生成一个3元素向量.这也就是`全连接层`的定义。

3.激活函数：除了第0层，每层必须需要激活函数，激活函数是`神经网络逼近拟合任意函数的关键`，如果不使用激活函数，无论多少层，都可以通过简单的数学运算矩阵运算简化为一层输入一层输出，激活函数引入了非线性。

- 非线性：横向考虑神经网络的前向传播，不存在激活函数的神经网络最终都可以理解为直线、超平面。使用激活函数后，在每个传播的过程中，因为激活函数是非线性不连续的，直线弯折，虽然不是“曲线”，但存在导数不存在的点，即构成了许多折线、小平面。那么许多小平面结合就具备了`拟合曲线、曲面的可能`。
- 竖向考虑每层中的不同神经元：每个神经元有自己独特的参数$(w,b)$，因此，`在面对同样的输入时，权重不同，相当于在不同方向上的投影输入，因此，不同神经元会对输入的不同特征组合“敏感”。`所以说，神经元 ≈ 特征提取器

- 除了第0层，每层必须需要激活函数

- | Sigmoid | 二分类问题输出层          |
  | :------ | ------------------------- |
  | Linear  | $y=x$什么都不做，回归问题 |
  | ReLU    | 大部分问题                |
  | Tanh    | 情感分析/文本分类         |
  | Softmax | 多分类，需要类别概率分布  |

4.Softmax：多分类问题$z=[z_1,z_2...z_k]$
$$
Softmax(z_i)= \frac{e^{z_i}}{\sum_{j=1}^Ke^{z_j}}\tag9
$$

- 注意：==Softmax仅用于多分类的输出层==，在采用Softmax的输出层也就不必再采用Relu。
- ==ReLu、Tanh、Sigmoid一般用于隐藏层==

5.高级优化损失函数：Adam：自动优化学习率，加快收敛。

6.额外的层类型：

- 卷积层：对信息做初步卷积再输入，将写输入原子结合作为更大的输入单位并提取特征。


==7.反向传播：==即梯度下降，将线性回归的梯度下降、Adam等做推广，自动优化每个神经元的$(w,b)$

##### 1.5 神经网络的度量

1.模型评估度量：最简单也是最有效的，划分测试集与训练集，分层抽样等，二者相互独立，但各自全面。

- ==训练集、交叉验证集、测试集==  
- 交叉验证集：用于选择模型的规模，计算交叉验证误差，选择泛化误差最低，泛化性能最好的。也就是说，`在测试集之外，额外选择一部分来调整超参数。`

2.诊断偏差与误差：通过训练集误差、测试集误差与交叉验证集误差，判断欠拟合、过拟合。

3.==高偏差->欠拟合;高方差->过拟合==

4.正则化参数：

- 正则化参数过大->欠拟合;正则化参数过小->过拟合
- 交叉验证集误差同样可以用于进行正侧化参数选择。核心就是，避免测试集在任何情况下提前加入训练。                                                                          

5.基准性能水平：

6.神经网络对于欠拟合、过拟合的解决办法：

- 欠拟合：逐步**增大模型规模**，同时注意适当正则化。
- 过拟合：**增加数据数量**、提升泛化能力。



##### 1.6 神经网络其余

1.`数据增强`：通过旋转图像、增加对比度，覆盖扭曲网格等，生成多张图像，同时具备区别，避免过拟合、提升泛化能力。总而言之，生成有区别的可用于训练的数据，尽量避免过拟合的可能性。

2.迁移学习：类似的领域中，`可以利用模型参数减少所需训练时间，同样的也就解决了其中一个领域数据较少的问题`，所需更新的参数变少、需要更新的次数变少。

- 例如，五层神经网络(X-X-X-X-100)，先用用于训练分类100类型的动物，而后，替换最后一层输出层为10层,来进行数字识别。前五层的训练参数可以直接固化，也可以作为初始值辅助模型更快的拟合。
- 这个过程，也成为==监督预训练==
- 实际上，你对别人的神经网络进行`微调`，也就是一种迁移学习。在别人的基础上开始。

3.混淆矩阵：一种用于评估分类模型性能的工具，它以矩阵形式展示**模型预测结果与真实标签的对应关系**。行通常表示真实类别，列表示预测类别。从而进行计算：准确率、精切率与召回率(真实正类中有多少被正确预测)



##### 1.7 决策树

1.决策树概述：`输出离散(分类、归(房价预测))，试图通过构建树的方式，以树的不同路径最终走到叶子节点来形成推理逻辑`。树的节点可以是一个值、或一个范围。

- 以选择不同的特征为根节点再一路向下生成为例，显然同样的特征可以组织出`多种多样的决策树`，因此，目标是试图找到最优的。
- 尽量找出节点，使分隔结果更为纯粹。即几类不同的数据，完全的互不交叉的分类在了几个叶子节点中。

2.纯度测量：用熵量化样本的纯度。

- ![image-20251013145323890](D:\Study\CS Study\Note 2024-\All Photo\image-20251013145323890.png)
- 一半一半最不纯粹，没有那个样本为主导一说。
- 在决策树中，选择哪种特征做分割，取决于哪种特征能够减少最多的熵，减少不纯度，最大化纯度

3.==独热(One-Hot)编码：处理离散性特征==。共X个类型，比如(圆形、方形、椭圆形、菱形)，则 => 四元组，每个1代表存在某种特性，输入向量。

4.连续值分隔：分割出不同的数据区间

5.回归树：用于预测连续值的决策树，因此决策树即可分类也可回归。利用叶子节点的区间均值或最小化平方误差的值。残差平方和RSS作为目标函数。

6.树集成：解决单棵树对微小数据敏感。原始分裂点依赖于选取的特征的数据。比如替换样本的一个属性内容，树会分化出完全不同的形状。

- 设计多棵树，其因为属性内容的不同具有完全不同的分裂节点。在多棵树之间采用投票的方式进行预测。
- 有回放采样：`又放回的构建随机集合——随机森林`

==7.随机森林算法：==每次随机有回访采样得到的训练集生成一颗随机树，最终构成随机森林。

- 进一步的，避免大数据集中，根节点及其附近节点被反复选择，即很多棵树的根节点相同，在每一个节点上随机化特征选择。
- 如果一共$N$个特征，算法令在$k=\sqrt{N}$中进行特征选择

8.XGBOOST：在选择数据集训练新的决策树的过程中，不再以$\frac{1}{m}$的纯粹随机概率进行选择，而是尽可能地选择在前面的决策树中表现不佳的误分类样本。



#### 2 Unsupervised Learning无监督学习

##### 2.1 Cluster 聚类

1.聚类概述：不存在目标标签y

==2.K均值聚类：==

- 起始：随机选择K个点作为初始簇中心
- 染色：将每个数据点分配到距离最近的质心所在的簇
- 更新：重新计算每个簇的质心
- 迭代：反复执行以上步骤，直至收敛

4.初始化K均值：

- 完全随机
- K-means++：第一个质心随机选择，后续质心按与已有质心的距离平方成比例进行选择。能显著减少收敛到局部最优的概率，提高聚类效果。
- 多次运行取最优

5.K值选择：常用肘部法则、轮廓系数、Gap Statistic等指标综合判断。



##### 2.2 Anomaly Detection 异常检测

1.异常检测概述：一种用于识别数据中**不符合正常模式或分布的样本**的技术，也称为离群点检测，它的目标是发现那些与大多数数据显著不同的点、事件或行为。

- 统计学方法：Z-score、箱线图、假设检验
- 基于距离/密度的方法：KNN 异常检测：计算点与邻居的距离，距离过大则为异常。局部异常因子（LOF）：比较点的局部密度与邻居的密度差异。
- 基于聚类的方法
- 基于机器学习的方法

##### 2.3 推荐系统

---

3.主成分分析(PCA)：一种常用的降维方法，通过线性变换将高维数据映射到低维空间，同时尽可能保留原始数据的方差信息。它的`核心思想是找到一组新的正交坐标轴（主成分），使得数据在这些方向上的投影方差最大`，从而实现数据压缩、去噪和特征提取。



##### 2.4 强化学习







































