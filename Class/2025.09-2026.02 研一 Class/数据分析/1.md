##### 1

1、 （简答题）查询资料，简述某个特定大语言模型预训练时采用的数据及其特点。

**简答：DeepSeek 在预训练阶段使用了约 14.8 万亿 tokens 的大规模、多样化、高质量语料，涵盖通用文本、代码、学术论文以及多语言数据，并通过严格的去重、过滤和质量控制机制，确保模型具备广泛的通用知识、较强的推理与代码能力，以及跨领域的适应性。**

## 🔎 DeepSeek 预训练数据及特点（以 DeepSeek-V3 为例）

### 1. 数据规模

- **规模极大**：DeepSeek-V3 使用了 **14.8 万亿 tokens** 进行预训练。
- 相比前代模型（DeepSeek-V2），数据量显著提升，保证了更强的语言建模能力。

### 2. 数据来源与类型

- **通用语料**：网页、百科、新闻、书籍、学术论文，覆盖科技、文学、社会等多领域。
- **代码数据**：大规模开源代码库，提升编程与推理能力。
- **数学语料**：专门引入数学推理数据集（如 OpenR1-Math-220k），增强逻辑与计算能力。
- **多语言数据**：不仅包含中文和英文，还扩展到其他语言，提升跨语言理解与生成能力。
- **垂直领域数据**：金融、教育、医疗、交通、法律等专业领域语料，增强专业知识覆盖。
- **社交媒体与对话数据**：保证模型在日常交流和开放域对话中的自然性。

### 3. 数据处理与质量控制

- **去重与清洗**：剔除重复、低质量或噪声文本。
- **启发式过滤**：利用规则和模型筛选高质量语料。
- **质量分类器**：训练专门的分类器对数据进行打分，确保语料可靠性。
- **平衡性**：在通用知识、代码、数学和多语言之间保持比例，避免模型偏科。

### 4. 预训练目标与优化

- **多 Token 预测 (MTP)**：不仅预测下一个词，还预测多个未来词，提高学习效率。
- **长上下文支持**：训练过程中逐步扩展上下文窗口，从 4K → 32K → 最终支持 **128K tokens**，增强长文本处理能力。
- **MoE 架构结合数据**：通过混合专家机制，部分专家专门处理代码、数学或多语言数据，提升效率与专业性。

### 5. 特点总结

- **规模空前**：14.8T tokens，覆盖面极广。
- **多样性强**：通用文本 + 代码 + 数学 + 多语言 + 垂直领域。
- **高质量**：严格清洗与筛选，保证知识可靠。
- **专业性突出**：特别强化数学、代码和专业领域知识。
- **长文本能力**：支持 128K 上下文，适合复杂推理与长文档任务。

✅ **总结**：DeepSeek 的预训练数据不仅规模庞大，而且在多样性、质量控制和专业性上都进行了精心设计。这使得模型既能处理通用自然语言任务，又能在数学推理、代码生成和专业领域应用中表现突出。

参考来源：CSDN《一文解读DeepSeek训练数据集》，53AI《以 DeepSeek-V3为例，理解 Pre-train 和 Post-train》53ai.com，搜狐《DeepSeek大模型是什么》，百度开发者中心《DeepSeek大语言模型技术架构与应用实践指南》百度开发者中心。

---

2.![image-20251019171009579](D:\Study\CS Study\Note 2024-\All Photo\image-20251019171009579.png)







---

3.

