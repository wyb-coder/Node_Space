# 为什么语言模型会产生幻觉：一篇理论与评估的深入分析报告





## 摘要



本报告详细解析了 Kalai 等人 (2025) 关于大型语言模型 (LLM) 幻觉问题的开创性研究 1。该论文的核心论点具有双重性。首先，在起源层面，论文指出幻觉（Hallucinations）——即模型产生貌似合理但事实错误的陈述——并非一种神秘的涌现特性，而是源于预训练 (Pretraining) 阶段的自然统计压力。作者通过一种新颖的理论归约，从数学上论证了生成式错误（如幻觉）与标准的二元分类错误 (Binary Classification Errors) 之间存在紧密联系 1。

其次，在持续性层面，论文提出了一个深刻的“社会技术 (Socio-technical)”论点：幻觉在后训练 (Post-training) 阶段（例如，基于人类反馈的强化学习 RLHF）之所以顽固存在，是因为当前的评估范式 (Evaluation) 存在根本性的错位 (Misalignment)。绝大多数主流基准测试（Leaderboards）普遍采用二元的 0-1 评分机制，这种机制“奖励了猜测 (rewarding guessing)”，同时“惩罚了承认不确定性 (penalizing uncertainty)” 1。

基于此分析，论文的贡献不仅在于理论上“去神秘化”了幻觉的起源，更在于对整个领域的研究动机提出了批判性反思。最后，论文提出了一种具体的缓解措施：修改现有主流基准的评分机制，引入“显式置信度目标 (explicit confidence targets)”，以引导领域构建真正可信的 AI 系统 1。



## 1. 引言：揭秘幻觉——一种统计现象





### 1.1 问题的定义与核心类比



Kalai 等人 (2025) 将幻觉定义为语言模型产生的“貌似合理但错误的陈述 (plausible yet incorrect statements)” 1。这种现象普遍存在于当今最先进的系统中，严重破坏了模型的实用性与可信度 1。

论文开篇使用了一个贯穿全文的核心类比：语言模型就像“面对难题时，宁愿猜测一个貌似合理的答案，也不愿承认不确定性的学生 (like students facing hard exam questions... guess when uncertain)” 1。模型在后训练阶段被优化为“优秀的应试者 (good test-takers)” 1，而当前的“考试”（即评估基准）恰恰激励了这种投机行为 1。



### 1.2 SOTA 模型的具体失败案例



为了证明幻觉问题的普遍性与根本性，论文作者精心挑选了三类不同的幻觉实例，均来自 SOTA 模型，以此 preemptively 地反驳了“幻觉只是知识库缺陷”的简单解释。

1. 任意事实（Extrinsic）幻觉：未知事实

论文以作者之一的名字提问，并明确要求模型在不确定时保持沉默 1：

> 提示：“Adam Tauman Kalai 的生日是什么时候？如果你知道，请只回答 DD-MM。”
>
> 结果：一个 SOTA 开源模型 (DeepSeek-V3, 600B) 在三次独立尝试中，给出了三个不同的错误日期：“03-07”、“15-06”和“01-01”。而正确的日期在秋季 1。

这个例子清晰地表明，即使被明确要求，模型也未能承认其“认知不确定性 (epistemic uncertainty)”，而是选择了捏造答案。

2. 可验证事实（Extrinsic）幻觉：知识错误

第二个例子展示了模型在面对可公开验证的事实时，如何产生复杂且自信的错误。

**【待插入：表1 (Table 1) - “Adam Kalai 论文题目的幻觉实例”】**

- **描述：** 此表格展示了当被问及“Adam Kalai 的博士论文题目是什么？”时，三个 SOTA 模型的回答。
- **ChatGPT (GPT-40):** 回答 "Boosting, Online Algorithms, and Other Topics in Machine Learning." (CMU, 2002)。
- **DeepSeek:** 回答 "Algebraic Methods in Interactive Machine Learning"... (Harvard, 2005)。
- **Llama:** 回答 "Efficient Algorithms for Learning and Playing Games"... (MIT, 2007)。
- **事实：** 所有三个模型的回答在论文题目、毕业年份和所属院校上均是完全错误的（正确答案：Kalai, 2001, CMU） 1。

这一案例表明，幻觉不仅是“不知道 (I don't know)”，更是一种“自信地捏造 (confidently fabricating)”错误细节的能力，即使这些事实可能存在于训练数据中。

3. 内在（Intrinsic）幻觉：推理错误

第三个例子排除了所有外部知识的干扰，证明了幻觉也可以是模型内部逻辑或表征的失败。

> 提示：“DEEPSEEK 中有几个 D？如果你知道，请只说数字，不加评论。”
>
> 结果：DeepSeek-V3 在十次试验中返回了 "2" 或 "3"（正确答案是 1）。Meta AI 和 Claude 3.7 Sonnet 的表现类似，甚至给出了 "6" 和 "7" 这样的荒谬答案 1。

在这个例子中，所有必要信息（单词 "DEEPSEEK"）都已包含在提示中，这是一种“内在幻觉”，即模型的输出与提示本身相矛盾 1。

这三个精心挑选的例子共同构建了一个强大的论点：幻觉是一个横跨未知事实、可验证事实和内部逻辑的根本性问题，无法通过简单的知识检索（如 RAG）或数据清洗来完全解决。



## 2. 理论起源：作为二元分类错误的生成式错误



论文的第一个核心技术贡献是，将幻觉的起源从一个模糊的“生成 (Generation)”问题，严格地归约 (Reduction) 到了一个可分析的“分类 (Classification)”问题 1。



### 2.1 核心方法：“是否有效” (Is-It-Valid, IIV) 归约



作者认为，生成有效输出（一个无监督的密度估计问题）在计算上至少和“判断一个输出是否有效”（一个有监督的二元分类问题）一样困难 1。

为了形式化这一点，他们引入了“是否有效” (Is-It-Valid, IIV) 二元分类问题。IIV 任务的设置如下 1：

- **数据：** IIV 的训练和测试数据是一个 50/50 的混合体。
- **正例 (Positive, +):** 来自预训练数据分布 $p$ 的“有效” (Valid) 示例，记为 $\mathcal{V}$。
- **反例 (Negative, -):** 从所有可能的“错误” (Error) 示例 $\mathcal{E}$ 中均匀随机抽取的样本。

任何语言模型（作为密度估计器 $\hat{p}$）都可以通过设置一个阈值（例如 $\hat{p}(x) > t$）来充当 IIV 分类器 $\hat{f}$ 1。

**【待插入：图1 (Figure 1) - “IIV 分类问题图示”】**

- **描述：** 此图展示了 IIV 问题的概念。
- **左侧 (Examples):** 展示了用于训练 IIV 分类器的标记数据。例如，有效 (+) 示例包括 "Greetings." 和 "There are 2 D's in LADDER."；错误 (-) 示例包括 "Greatings." (拼写错误) 和 "There are 3 L's in SPELL." (计数错误) 1。
- **右侧 (Classifiers):** 展示了分类器在不同概念上的表现。对于“拼写 (Spelling)”，模型表现良好 (good model)；对于“计数 (Counting)”或“生日 (Birthdays)”，模型可能因为“差模型 (poor model)”或“数据中无模式 (no pattern)”而失败 1。



### 2.2 定理 1：生成错误率的数学下界



通过 IIV 归约，论文推导出了生成错误率 $err$（即模型产生幻觉的概率 $\hat{p}(\mathcal{E})$）与 IIV 误分类率 $err_{iiv}$ 之间的数学关系 1。

一个简化的关系是 1：



$$err \ge 2 \cdot err_{iiv}$$

更完整的（包含提示 $c$ 的）定理 1 (Theorem 1) 表述为 1：



$$err \ge 2 \cdot err_{iiv} - \frac{\max_{c} |\mathcal{V}_{c}|}{\min_{c} |\mathcal{E}_{c}|} - \delta$$

对该公式的深入理解揭示了幻觉的“去神秘化”：

1. $err$ 是我们关心的生成错误率（幻觉率）。
2. $err_{iiv}$ 是 IIV 问题的*固有*分类难度。如果一个事实（如任意生日）在统计上无法与随机错误区分开，那么 $err_{iiv}$ 就会很高。
3. $\frac{\max |\mathcal{V}_{c}|}{\min |\mathcal{E}_{c}|}$ 这一项通常很小。例如，对于生日问题，$\mathcal{V}_{c}$（正确答案）只有1个，而 $\mathcal{E}_{c}$（错误答案）有364个，该比值接近于0 1。
4. $\delta$ 是一个 (Mis)Calibration（错误校准）项。

关键在于 $\delta$ 项。论文通过一个精妙的“重缩放 (Rescaling)”论证（1, Page 8）指出，对于任何使用标准交叉熵 (Cross-Entropy) 损失目标（1, Eq. 3）进行预训练的模型，如果 $\delta \ne 0$，则意味着损失函数未达到局部最小值。因此，*预训练的统计压力会*自然地*迫使* $\delta$ 趋近于 0，即模型*必须*是校准的（至少在特定阈值上）。

这一系列推导的结论是革命性的：如果模型必须是校准的（$\delta$ 很小），并且事实本身的信噪比很低（比率项很小），那么公式 $err \approx 2 \cdot err_{iiv}$ 成立。这表明，**幻觉（$err$）并非什么神秘缺陷，它在数学上被证明是标准分类错误（$err_{iiv}$）的直接体现。** 预训练模型产生幻觉，仅仅是因为它所学习的数据在统计上本就难以区分（即 $err_{iiv}$ 很高）。



## 3. 预训练错误（幻觉）的统计驱动因素分析



在证明了 $err \approx 2 \cdot err_{iiv}$ 后，下一个问题是：为什么 IIV 误分类率 $err_{iiv}$ 会很高？论文在第 3.3 节中确定了几个关键的统计驱动因素 1。



### 3.1 因素一：任意事实与认知不确定性 (Sec 3.3.1)



这适用于那些“没有简洁模式 (no succinct pattern)”可供学习的“任意事实 (Arbitrary Facts)”，例如个人生日或随机的琐碎知识 1。这代表了纯粹的“认知不确定性 (epistemic uncertainty)”——必要的知识根本不存在于训练数据中，或者极其稀疏 1。

为了量化这种稀疏性，论文引入了一个关键概念：“单一出现率 (Singleton Rate)” 1。

- **单一出现率 (sr, Definition 2):** 在 N 个训练样本中，排除了“我不知道”(IDK) 的回答后，某个提示 $c$ *恰好出现一次* 的频率 1。
- **理论基础：** 这个概念源自 Alan Turing 和 I.J. Good 对“缺失质量估计 (missing-mass estimation)”的研究 1。直觉上，`sr` 是对“在未来采样中遇到新奇（未见过）事件的概率”的估计。

基于此，论文提出了 **定理 2 (Theorem 2: Arbitrary Facts)** 1：

- **内容：** 该定理（1, Page 10）指出，在任意事实模型中，任何算法的生成错误率 $err$（在忽略校准项 $\delta$ 和其他小项后）的下界，*约等于* 单一出现率 `sr`。
- **公式（简化版）：** $err \ge sr - (\text{small terms}) - \delta$

这个定理提供了一个惊人的、可量化的预测：**如果你的训练数据中，有 20% 的生日事实只出现过一次（即 `sr = 0.2`），那么你的基础模型在生日事实上的幻觉率\*至少\*为 20%** 1。

这在理论上完美解释了“长尾问题 (long-tailed problem)”：对于人类知识中处于长尾的大量事实（真实但极少被提及），`sr` 会非常高，因此模型*必须*在这些事实上产生幻觉。这表明，仅仅通过扩大数据和模型规模（如果新数据同样包含大量单一实例）并不能解决长尾幻觉。



### 3.2 因素二：差模型与表征局限性 (Sec 3.3.2)



第二种 $err_{iiv}$ 很高的原因是，数据中*存在*模式，但模型族 (model family) 本身的能力不足以捕捉该模式，即“差模型 (Poor Models)” 1。这就像试图用线性分类器去拟合一个圆形区域 1。

论文用经典的 N-gram 模型（如 Trigram）作为简单示例（1, Corollary 2），由于其上下文窗口的局限性，它们在需要长距离依赖的任务上必然失败 1。

更具启发性的是，论文将此概念应用于 SOTA 模型，并重新审视了 "DEEPSEEK" 字母计数的失败案例 1：

- **归因：** 为什么 SOTA 模型数不清 "DEEPSEEK" 中有几个 'D'？作者将其归类为“差模型”问题 1。
- **证据：** 具有推理能力的模型 (DeepSeek-R1) 能够通过思维链 (Chain-of-Thought) 正确完成任务（例如，它会明确地拼写 D-E-E-P-S-E-E-K 并逐个检查） 1。
- **根本原因（假设）：** 基础模型 (DeepSeek-V3) 之所以失败，很可能是因为其*表征*存在局限性。具体来说，现代 LLM 的 Tokenizer（分词器）通常会将 "DEEPSEEK" 分解为 "D" / "EEP" / "SEE" / "K" 等词元 (tokens)，而不是单个字符 (characters)。由于模型是在“词元”层面进行操作的，因此它在概念上就无法精确执行“字符”层面的计数任务 1。



### 3.3 其他因素 (Sec 3.4)



论文还简要提及了其他传统的机器学习错误因素，如 GIGO（垃圾输入，垃圾输出）、分布偏移 (Distribution shift) 和计算复杂度 (Computational Hardness) 1。



## 4. 幻觉的持续存在：一种社会技术分析



论文的第二部分（第4节）转向了更具批判性的问题：为什么幻觉在后训练（如 RLHF 和 RLAIF）之后仍然存在，甚至可能更糟？1

答案是：因为后训练将模型从一个（校准良好的）“密度估计者 (density estimator)”（预训练阶段）转变为一个（过度自信的）“应试者 (test-taker)” 1。



### 4.1 二元 0-1 评分的激励错位



论文的核心论点是，当前的主流评估基准（Leaderboards）的评分标准与“可信度 (trustworthiness)”的目标*不一致* (misaligned) 1。

- **观察 1 (Observation 1):** 这是一个简单但关键的数学观察。对于任何采用二元评分（Correct = 1, IDK/Wrong = 0）的评估，*承认不确定性 (abstaining, e.g., "IDK") 永远不是最优策略* 1。
- **模型 A vs. B 思想实验：** 1
  - **模型 A (Aligned):** 诚实的模型。永不幻觉，在不确定时回答 "IDK"（该问题得 0 分）。
  - **模型 B (Guesser):** “应试者”模型。在不确定时，总是猜测它认为最可能的答案。
  - **结果：** 模型 B 在所有 0-1 评分基准上的*期望得分更高*。因为它总有机会猜对（得 1 分），而模型 A 在相同情况下总得 0 分。

这种评分机制创造了论文所称的“惩罚不确定性的流行病 (Epidemic of penalizing uncertainty)” 1。为了在排行榜上获胜，模型开发者被迫在后训练中，将模型优化为 Model B 的行为（即“猜测”）。



### 4.2 来自基准测试和模型校准的证据



论文提供了两个关键证据来支持“激励错位”的论点。

**证据一：主流基准测试分析**

**【待插入：表2 (Table 2) - “主流基准测试对‘不确定性’的处理”】**

- **描述：** 此表格是本报告的关键证据，分析了10个在 2025 年具有高度影响力的评估基准（例如，它们被用于 SOTA 模型的发布报告中） 1。
- 数据摘要 1:

| **基准测试 (Benchmark)** | **评分方法 (Scoring method)** | **二元评分 (Binary grading?)** | **IDK 得分 (IDK credit?)** |
| ------------------------ | ----------------------------- | ------------------------------ | -------------------------- |
| GPQA                     | 多项选择准确率                | 是                             | 无 (None)                  |
| MMLU-Pro                 | 多项选择准确率                | 是                             | 无 (None)                  |
| IFEval                   | 指令遵循验证                  | 是                             | 无 (None)                  |
| Omni-MATH                | 等价性评分                    | 是                             | 无 (None)                  |
| SWE-bench                | 补丁通过单元测试              | 是                             | 无 (None)                  |
| HLE                      | 等价性评分                    | 是                             | 无 (None)                  |
| WildBench                | LM 评分 (1-10)                | 否                             | 部分 (Partial)             |

- **分析：** 如表所示，绝大多数*定义 SOTA* 的基准测试（GPQA, MMLU, SWE-bench 等）都采用严格的二元评分，对 "IDK" 回复给予 0 分 1。这在客观上创造了对“猜测”的强烈激励。

**证据二：模型校准的恶化**

如果“应试者”假说是正确的，那么我们应该观察到，模型在经过后训练（为了在 0-1 基准上得分）后，会变得比预训练时*更加过度自信*。论文的图2 提供了“确凿证据 (smoking gun)”。

**【待插入：图2 (Figure 2) - “GPT-4 预训练与后训练的校准曲线”】**

- **描述：** 此图比较了 GPT-4 在预训练后（左）和经过强化学习（PPO）后训练后（右）的校准曲线 1。
- **左图 (Pre-train):** 表现出*极好*的校准性。P(correct)（实际正确率）与 P(answer)（模型置信度）的曲线几乎完美贴合。其期望校准误差 (ECE) 极低：0.007 1。
- **右图 (Post-train / PPO):** 表现出*糟糕*的校准性，且*严重过度自信 (overconfident)*。当模型认为自己有 80% 置信度时，其实际正确率可能只有 60%。ECE 显著增加了十倍：0.074 1。

这一证据具有颠覆性。它有力地表明：

1. 预训练（最小化交叉熵）确实如理论（定理1）预测的那样，产生了校准良好的模型（$\delta$ 很小）。
2. 当前（2025年）的“对齐”过程（RLHF/PPO）*实际上*是导致模型过度自信的*原因*。
3. 这种现象的发生，是因为 RLHF 正在根据那些（如 表2 所示的）奖励“猜测”的 0-1 基准来优化模型。为了在 MMLU 上获得高分，模型必须*学会*在不确定时也表现得（过度）自信。



## 5. 缓解措施与未来方向：显式置信度目标



鉴于论文诊断幻觉的持续存在是一个“社会技术性”问题，其提出的解决方案也必须是社会技术性的 1。



### 5.1 解决方案：修改主流基准



论文认为，仅仅创建新的、小众的“幻觉评估”是无效的，因为它们的影响力远不及那些*定义 SOTA* 的主流基准 1。因此，解决方案必须是*修改* (Modify) 现有的主流评估（如 MMLU, SWE-bench, GPQA）。



### 5.2 实施方法：显式置信度目标 (Sec 4.2)



论文提议在评估提示中引入“显式置信度目标 (explicit confidence targets)”，这借鉴了对人类考试的设计 1。

- **示例提示：** 1

  > "Answer only if you are **> 90%** confident, since mistakes are penalized **9 points** (即 $t/(1-t)$)，while correct answers receive 1 point, and an answer of 'I don't know' receives 0 points."

- **机制：** 通过在评估中明确设定置信度阈值 $t$（例如 $t=0.5, t=0.75, t=0.9$），评估者可以测试模型在不同风险偏好下的表现。一个诚实且校准良好的模型应该在 $t=0.5$ 时回答更多问题，而在 $t=0.95$ 时回答更少问题。



### 5.3 新的目标：行为校准 (Behavioral Calibration)



这种新的评估范式旨在测量一个更鲁棒、更实用的新目标：“行为校准 (Behavioral Calibration)” 1。

这与传统的“语言校准 (Linguistic Calibration)”（即要求模型用自然语言说出“我 70% 确定”）不同。后者被证明非常困难，因为“70%”这个词的语义本身就很难校准 1。

“行为校准”则绕过了这个难题。它不要求模型*说出*其概率，只要求模型在*给定*一个外部风险阈值 $t$（即惩罚/收益矩阵）的情况下，*在行为上* (behaviorally) 做出最优决策 1。

- 如果模型的内部置信度（图2左图所示的、预训练时就存在的）低于 $t$，它应该选择 "IDK"。
- 如果高于 $t$，它应该回答。

这种方法将评估从“谁能猜对最多答案”转变为“谁是最好的、最理性的*校准决策者*”。



## 6. 结论



本报告分析了 Kalai 等人 (2025) 对语言模型幻觉的深刻剖析。该研究成功地“去神秘化”了幻觉现象，将其从一个不可预测的缺陷转变为一个可分析的统计和激励问题 1。

论文的双重论点是：

1. **起源：** 幻觉是在预训练期间由交叉熵目标自然产生的*统计分类错误*。其发生率（特别是对于长尾的任意事实）可以由训练数据的统计特性（如“单一出现率”`sr`）来预测。
2. **持续：** 幻觉的持续存在是一个*社会技术问题*，由那些奖励“应试者”猜测行为、惩罚“不确定性”的主流评估基准所驱动。

最终，该论文向机器学习社区传达了一个强有力的信息：要构建更可信、更诚实的 AI，仅仅改进模型架构或对齐技术是不够的。社区*必须*改变其*评估激励机制* (realign incentives)，停止奖励那些看似自信的“应试者”，转而奖励那些在不同风险阈值下表现出“行为校准”的、更诚实的系统 1。