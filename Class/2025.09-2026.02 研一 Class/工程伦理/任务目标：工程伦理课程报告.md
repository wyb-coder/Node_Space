# AI决策系统中的“黑箱”与公众知情权：一项工程伦理责任框架



**摘要**

随着人工智能（AI）决策系统在司法、医疗、金融等高风险公共领域的广泛应用，其内部决策逻辑不透明的“黑箱”问题日益凸显。本文认为，AI系统的“黑箱”不仅是一个技术挑战，更是一个严峻的工程伦理问题，它直接挑战了公众在切身利益受影响时所享有的基本知情权。本文首先剖析了“黑箱”问题的技术与伦理双重维度，并以美国司法领域广泛使用的COMPAS算法为例，深度分析了其在实践中暴露的算法偏见与公平性争议。紧接着，本文探讨了医疗AI领域中可解释性的缺失如何侵蚀医患信任与医疗责任。在此基础上，本文的核心贡献在于提出一个面向工程伦理的“责任透明度”三维框架，该框架整合了技术可解释性（如LIME、SHAP等XAI工具）、流程规范性（如算法影响评估AIA）与责任可追溯性（如“伦理黑匣子”与IEEE伦理设计准则），旨在为工程师提供一套可操作的伦理实践指南。本研究预期，通过实施此框架，能够增强AI系统的公信力，保障公众的合法权益，并最终将透明度确立为工程师在AI时代不可推卸的核心伦理责任。

**关键词**

工程伦理；人工智能；黑箱问题；公众知情权；算法偏见；可解释性；责任



## 1 引言





### 1.1 研究背景：算法决策时代的伦理困境



我们正步入一个由算法驱动决策的时代。人工智能（AI）与机器学习模型正以前所未有的深度和广度渗透到社会运行的各个层面，尤其是在司法裁决、医疗诊断、金融信贷等高风险公共领域，这些系统的决策直接关系到个体的自由、健康与财产安全 1。理论上，这些系统被期望能够带来更高的效率、客观性和一致性。然而，伴随其强大能力而来的，是一种深刻的伦理困境：这些系统的核心决策过程往往是一个“黑箱” 。

这种操作上的不透明性源于深度学习等模型内部数以百万计参数的极端复杂性，其决策路径往往连其创造者也难以完全追溯和理解 1。这种不透明性带来了巨大的风险，包括但不限于：固化并放大训练数据中潜藏的历史性社会偏见，导致歧视性结果；在系统出错时，责任归属变得模糊不清，削弱了问责机制；最终，公众对这些关乎其命运的自动化系统的信任也随之瓦解 1。这一困境并非遥远的未来猜想，而是已经广泛部署的现实系统所持续引发的伦理争议 3。这种技术范式（以复杂性换取准确性）与公共领域所要求的透明、可问责的民主价值观之间，存在着一种根本性的、系统性的冲突，这已然超出了纯粹的技术范畴，演变为一个亟待解决的工程伦理难题。



### 1.2 案例引入与核心问题



为了具象化这一伦理困境，本文将聚焦于两个典型的应用场景，它们集中体现了“黑箱”问题对公共利益的冲击。

第一个案例来自刑事司法领域。COMPAS（Correctional Offender Management Profiling for Alternative Sanctions）算法在美国多个州的司法系统中被用于评估被告的再犯风险，其评估结果会影响保释、量刑乃至假释决策 。然而，2016年非营利调查新闻机构ProPublica的一项深入调查揭示，该算法在预测中存在显著的种族偏见，对黑人被告的误判率远高于白人被告，引发了关于算法公平与正义的激烈辩论 9。

第二个案例源于医疗健康领域。AI驱动的诊断工具在识别医学影像（如X光片、MRI）中的病灶方面展现出巨大潜力，但其“黑箱”特性同样引发了伦理关切。当一个AI系统建议某种治疗方案或诊断结果时，如果医生无法理解其背后的医学逻辑，他们将难以信任、验证或向患者解释这一决策，这直接影响到患者的知情同意权、治疗安全以及医疗责任的划分 1。

基于以上背景与案例，本文提出核心研究问题：**当具有“黑箱”特性的人工智能决策系统影响到公共利益时，其不透明性是否侵犯了公众的基本知情权？我们应如何将这一冲突重新界定并作为一个核心的工程伦理责任来加以应对？** 本文的创新点在于，明确地将“透明性”从一个单纯的技术属性问题，提升为一个关乎风险、安全与责任的工程伦理核心议题 。



## 2 相关工作：透明性、偏见与知情权的交叉审视





### 2.1 “黑箱”问题的技术与伦理双重维度



“黑箱”AI系统，其定义为用户能够观察到系统的输入与输出，但无法洞察其内部转化过程的系统 。这一现象的产生具有技术与商业的双重根源。

从技术维度看，不透明性主要源于现代机器学习模型，特别是深度神经网络的内在复杂性。这些模型通过在海量数据中学习高度非线性的复杂模式来获得卓越的预测性能，其内部包含数百万甚至数十亿的参数，使得任何单一决策的完整数学路径都变得几乎无法追踪和解释 1。

从商业与法律维度看，不透明性也常常是一种刻意的选择。许多AI算法作为企业的核心知识产权受到商业秘密的保护，公司以此为由拒绝公开其内部工作原理，从而阻碍了公众和监管机构的有效监督 2。

这种双重因素导致的不透明性直接催生了一系列严重的伦理后果：

- **侵蚀信任与问责**：当决策过程无法被理解时，公众和使用者便无法建立信任。一旦系统出现错误决策并造成伤害（例如，自动驾驶汽车事故），确定责任方——是开发者、数据提供者还是使用者——变得异常困难，问责机制形同虚设 1。
- **隐藏与放大偏见**：不透明的模型极易成为偏见的“庇护所”。如果训练数据中包含了历史上存在的社会偏见（如种族、性别歧视），模型将在不为人知的情况下学习并固化这些偏见，甚至在应用中将其放大，导致系统性的不公平 2。
- **阻碍调试与修正**：当一个“黑箱”系统持续产生错误或有害的输出时，由于无法洞察其内部逻辑，工程师很难精确定位问题根源并进行有效修复，这在安全攸关的领域是致命的 2。



### 2.2 算法偏见的实证研究：COMPAS案例深度剖析



COMPAS算法的争议为“黑箱”问题及其伦理后果提供了迄今为止最深刻和最具影响力的实证案例。

2016年，ProPublica发表了题为“机器偏见”（Machine Bias）的调查报告，通过分析佛罗里达州布劳沃德县超过7000名被告的COMPAS风险评分及其后两年的犯罪记录，得出了惊人的结论：该算法在预测错误上表现出显著的种族差异 3。

报告的核心统计数据显示：

- **错误的“高风险”标签（假阳性）**：在最终没有再犯的被告中，黑人被告被错误地标记为“高风险”的比例（44.9%）几乎是白人被告（23.5%）的两倍。
- **错误的“低风险”标签（假阴性）**：在最终实施了再犯的被告中，白人被告被错误地标记为“低风险”的比例（47.7%）远高于黑人被告（28.0%） 3。

这些数据揭示了一个系统性问题：COMPAS算法的错误倾向于对黑人被告更为不利，而对白人被告更为“宽容”。

“公平”的定义之争

该调查引发了关于如何定义“算法公平”的激烈辩论，其核心在于两种相互冲突的公平标准。

| **公平性指标**                     | **定义**                                                     | **支持方论点（为何“公平”）**                                 | **关键影响/后果**                                            |
| ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **预测均等性 (Predictive Parity)** | 对于任何给定的风险分数（例如7分），黑人与白人被告的实际再犯概率是相同的。 | **Northpointe公司**：分数对所有种族都具有同等的预测准确性，是无偏的预测工具 [10, 20]。 | 接受了不平等的错误率，意味着某个群体（黑人）将不成比例地承担更多被错误标记为“高风险”的代价 [17]。 |
| **均等化赔率 (Equalized Odds)**    | 不同种族群体在被错误预测为“高风险”（假阳性率）和被错误预测为“低风险”（假阴性率）上的比例应该是相等的。 | **ProPublica**：防止任何一个群体因算法的错误而受到不成比例的惩罚或伤害，保障了程序的公平性 [10, 21]。 | 导致一个给定的风险分数对不同群体意味着不同的实际再犯风险，牺牲了预测的一致性 [17, 22]。 |

数学上的僵局与伦理选择

随后的学术研究进一步揭示了一个更深层次的问题：当两个群体的基础再犯率（base rates）本身就存在差异时，任何算法在数学上都不可能同时满足“预测均等性”和“均等化赔率”这两个公平标准 17。这意味着，在一个反映了社会不平等的数据库中，“算法公平”并非一个可以被完美求解的客观技术问题，而是一个必须在不同类型的伤害之间做出权衡的伦理选择。工程师无法通过技术手段“消除”偏见，而只能选择让哪个群体来承担哪种类型的统计误差。这一发现从根本上将算法设计问题从技术领域推向了伦理与价值判断的前沿。

更具颠覆性的是，后续研究表明，COMPAS算法的预测准确率（约65%）甚至不比没有专业背景的普通人通过在线问卷做出的预测更准确 19。这使得人们对其应用的合理性产生了根本性质疑：我们为何要在一个事关公民自由的关键决策点上，部署一个如此复杂、不透明、充满争议且效果平平的系统？



### 2.3 算法时代的知情权：从法律原则到实践挑战



面对算法决策带来的不透明性，保障公民的知情权已成为全球法律与政策制定的焦点。这一权利的核心体现为“解释权”（right to explanation），即个人有权在受到自动化决策显著影响时，获得关于该决策逻辑的有意义的解释 27。

欧盟的《通用数据保护条例》（GDPR）是这一领域的先行者，其条款暗示了用户有权获得关于自动化决策逻辑的信息。而于近期通过的《人工智能法案》（EU AI Act）则更为明确，其第86条规定，对于被列入附件三的高风险AI系统（涵盖司法、就业、公共服务等领域）所做出的、对个人产生法律效力或类似重大影响的决策，受影响的个人有权从部署者那里获得“关于AI系统在决策程序中的作用以及所做决策主要内容的清晰且有意义的解释” 7。

然而，从法律条文到工程实践之间存在巨大的鸿沟。法律上的“解释权”并不能自动解决技术上的“黑箱”问题。它带来了一系列棘手的实践挑战：对于一个复杂的神经网络模型，什么才构成对普通用户“有意义的”解释？是提供数百万个模型权重，还是一个可能过度简化甚至误导的比喻？当算法受商业秘密保护时，如何强制企业履行解释义务，同时又不损害其知识产权？ 7。这种法律权利与技术能力之间的脱节，构成了一个关键的失效点，凸显了仅靠法律框架不足以解决问题，必须有相应的工程实践和伦理标准来填补这一空白。



## 3 本文贡献：构建面向工程伦理的“责任透明度”框架





### 3.1 工程伦理新范式：将透明度视为核心安全与责任要素



鉴于“黑箱”AI在公共领域应用的深刻影响，本文主张，必须将“透明度”从一项技术期望提升为工程伦理的核心原则，其重要性应与传统的公共健康、安全和福祉（public health, safety, and welfare）等基本准则等同 30。

这种范式转换可以类比于土木工程领域：正如一名土木工程师对其设计的桥梁的结构完整性负有绝对责任，并且必须能够通过可验证的计算和设计文档来证明其安全性一样，一名AI工程师也必须对其设计的算法的“信息完整性”和“决策公正性”负责，并有义务解释其决策的依据。一个无法解释其决策逻辑的AI系统，就如同一座无法说明其承重原理的桥梁，其在公共领域的应用本身就构成了不可接受的伦理风险。

这一主张与《工程伦理》课程大纲的核心精神紧密相连 。首先，根据课程**通论第二章“工程中的风险、安全与责任”**，一个不透明的“黑箱”系统本身就是一个巨大的**风险**源，因为它隐藏了潜在的偏见、错误和失效模式。因此，确保透明度是工程师管理**安全**、履行**责任**的基本行为。其次，根据**分论第十章“信息与大数据伦理问题”**，当利用大数据训练AI模型并用于对人进行评判和决策时，提供决策的透明度和解释性，是数据使用者和技术开发者对数据主体（即公众）应尽的核心伦理义务 32。

美国国家专业工程师协会（NSPE）等权威机构也强调，AI的出现非但没有减轻工程师的伦理责任，反而极大地**放大**了它。工程师不能将专业判断的责任推卸给算法，而必须成为AI工具的审慎评估者和伦理守门人 30。



### 3.2 “责任透明度”三维框架



为了将上述伦理原则转化为可操作的工程实践，本文提出一个包含三个层次的“责任透明度”（Responsible Transparency）框架。该框架并非单一解决方案，而是一个整合了技术工具、组织流程和治理保障的综合体系。



#### 3.2.1 技术可解释性 (Technical Explainability): 工程师的工具箱



这一层次关注工程师可以直接使用的、用以揭示“黑箱”内部运作的技术手段。其核心是**可解释性AI（Explainable AI, XAI）**，这是一系列旨在使模型决策过程更易于人类理解的方法 11。对于已经构建的复杂模型，工程师可以采用“事后”（post-hoc）解释技术，其中两种主流的“模型无关”（model-agnostic）方法是LIME和SHAP：

- **LIME (Local Interpretable Model-agnostic Explanations)**：该技术通过在单个预测实例的周围生成大量扰动数据点，然后用一个简单的、可解释的模型（如线性回归）来拟合“黑箱”模型在这一局部区域的行为。其结果可以直观地告诉我们，对于这一个案，哪些特征对最终决策起到了正面或负面的推动作用 37。
- **SHAP (SHapley Additive exPlanations)**：该技术借鉴了博弈论中的夏普利值（Shapley values）概念，通过计算每个特征在所有可能的特征组合中的边际贡献，从而为单个预测公平地分配每个特征的“贡献度”。SHAP值能够量化地展示出每个特征将预测结果从基准值推向最终值的具体力度 35。

在工程实践中，XAI工具应被视为高风险AI系统验证、审计和调试的标准组成部分。它们不仅能帮助工程师发现和诊断模型中的偏见，还能为受决策影响的个体生成事后解释，初步满足其知情权要求 11。



#### 3.2.2 流程规范性 (Procedural Regularity): 组织的强制规定



技术工具本身不足以保障伦理，必须嵌入到规范化的组织流程中。这一层次强调，透明度必须是贯穿AI项目整个生命周期的强制性要求，而非事后补救。实现这一目标的关键机制是**算法影响评估（Algorithmic Impact Assessment, AIA）** 43。

AIA是一种结构化的风险评估流程，旨在系统地识别、评估和记录AI系统可能带来的社会与伦理影响。AI Now研究所提出的AIA框架包含几个关键步骤：机构首先进行内部自评，评估系统对公平、正义的潜在影响；然后引入外部研究人员进行独立审查；最关键的是，机构必须在系统采购或部署**之前**向公众发布公告，说明系统的用途、能力和已知的风险，并公开征求公众意见 43。

加拿大政府的《自动化决策指令》为AIA的强制实施提供了现实样板。该指令要求所有联邦政府部门在使用新的自动化决策系统前，必须完成并公开发布AIA。AIA是一个公开的在线问卷工具，它根据系统的风险领域（如对个人权利、健康、经济利益的影响）评定出四个影响等级，并强制要求部署方根据不同等级采取相应的缓解措施，例如增强透明度通知、保障人工干预渠道、进行同行评审等 47。

AIA将伦理考量从技术人员的个人责任，转变为组织层面必须履行的、公开透明的法律和行政义务，确保了在潜在伤害发生前进行前瞻性的风险管理和公众参与。



#### 3.2.3 责任可追溯性 (Accountability Traceability): 治理的最后防线



考虑到商业秘密等因素可能限制完全的技术透明，必须建立一个即使在系统发生故障后也能确保责任可追溯的治理机制。这一层次引入了**“伦理黑匣子”（Ethical Black Box）**的概念 51。

这个概念类似于飞机上的飞行数据记录仪（Flight Data Recorder）。它是一个嵌入AI系统内部的安全、标准化的数据记录模块，持续记录系统运行的关键信息，例如：接收的输入数据、决策过程中的关键中间状态、最终的输出结果以及系统依据的决策规则或置信度。这些记录数据在平时是加密且不可访问的，只有在发生严重事故或争议时，由获得授权的独立审计机构或监管部门进行调取，用于事后调查、原因分析和责任认定 51。

“伦理黑匣子”为实现电气与电子工程师协会（IEEE）提出的《伦理协同设计》（Ethically Aligned Design）中的核心原则提供了具体的工程实现路径。IEEE的**原则五（透明性）**要求“A/IS特定决策的基础应始终是可发现的”，**原则六（问责制）**要求“为所有做出的决策提供明确的基本原理” 53。“伦理黑匣子”正是实现这种“可发现性”和“可问责性”的最后防线，它在保护企业知识产权和维护重大公共利益之间取得了一种务实的平衡。



## 4 预期结果与展望



采纳并实施本文提出的“责任透明度”三维框架，有望在多个层面带来积极且深远的影响。

首先，**对于工程师和工程行业而言**，该框架将伦理原则转化为具体的、可操作的实践指南。它超越了“不做恶”等模糊的口号，为工程师提供了在日常工作中评估、缓解和记录伦理风险的工具（XAI）、流程（AIA）和标准（伦理黑匣子与IEEE准则）。这将有助于提升工程师的伦理决策能力，强化其作为社会公共安全守护者的专业角色 30。

其次，**对于公众和受AI决策影响的个体而言**，该框架是保障其基本权利的坚实屏障。通过强制性的信息公开、解释机制和问责路径，公众对政府和企业使用的AI系统将建立起必要的信任。当决策过程变得更加透明、可审查、可申诉时，公民的知情权、公平权和寻求救济的权利才能得到有效保障 8。

最后，**对于整个社会而言**，该框架的推广有助于引导AI技术朝着更加公正、公平和负责任的方向发展。它通过制度化的设计，抑制了由算法偏见和错误所可能造成的社会危害，促进了一种以人为本的技术创新文化，确保AI的发展真正服务于增进人类福祉的终极目标 59。

当然，该框架的实施也面临诸多挑战，包括企业对公开商业秘密的抵触、实施AIA和伦理黑匣子所需增加的成本、相关法律法规和行业标准的滞后，以及对工程师进行跨学科伦理培训的迫切需求。

展望未来，AI“黑箱”问题将持续成为科技与社会交汇处的焦点。它不仅是对技术能力的考验，更是对工程职业伦理的终极拷问。工程师作为这些强大系统的设计者和建造者，身处塑造未来的关键位置。通过拥抱“责任透明度”框架，将伦理考量深度融入技术实践，工程界才能无愧于其对社会的基本承诺，确保我们创造的技术不仅是智能的，更是智慧、公正和值得信赖的。



## 参考文献



1. Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine Bias: There's Software Used Across the Country to Predict Future Criminals. And It's Biased Against Blacks. *ProPublica*.

2. Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. *Big data*, 5(2), 153-163.

3. Dressel, J., & Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism. *Science Advances*, 4(1), eaao5580.

4. European Commission. (2021). *Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act)*.

5. Goodman, B., & Flaxman, S. (2017). European Union regulations on algorithmic decision-making and a "right to explanation". *AI Magazine*, 38(3), 50-57.

6. IEEE. (2019). *Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems, First Edition*.

7. Kleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. *arXiv preprint arXiv:1609.05807*.

8. London, A. J. (2019). Artificial intelligence and black-box medical decisions: accuracy versus explainability. *Hastings Center Report*, 49(1), 15-21.

9. Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. In *Advances in neural information processing systems* (pp. 4765-4774).

10. National Society of Professional Engineers (NSPE). (2023). *NSPE Position Statement No. 1779 - Artificial Intelligence*.

11. Reisman, D., Schultz, J., Crawford, K., & Whittaker, M. (2018). *Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability*. AI Now Institute.

12. Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why should I trust you?": Explaining the predictions of any classifier. In *Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining* (pp. 1135-1144).

13. Treasury Board of Canada Secretariat. (2019). *Directive on Automated Decision-Making*.

14. Winfield, A. F., & Jirotka, M. (2018). Ethical governance is the heart of responsible robotics. In *The political economy of robotics and AI*. Edward Elgar Publishing.

15. Zwitter, A. (2014). Big data ethics. *Big Data & Society*, 1(2), 2053951714559253.









1. LONDON A J. Artificial intelligence and black-box medical decisions: accuracy versus explainability[J]. Hastings Center Report, 2019, 49(1):15-21.
2. IEEE. Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems, First Edition[S]. 2019.
3. ANGWIN J, LARSON J, MATTU S, KIRCHNER L. Machine Bias: There's Software Used Across the Country to Predict Future Criminals. And It's Biased Against Blacks[EB/OL]. ProPublica, 2016.
4. CHOULDECHOVA A. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments[J]. Big Data, 2017, 5(2):153-163.
5. KLEINBERG J, MULLAINATHAN S, RAGHAVAN M. Inherent trade-offs in the fair determination of risk scores[EB/OL]. arXiv:1609.05807, 2016.
6. EUROPEAN COMMISSION. Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act)[S]. 2021.
7. GOODMAN B, FLAXMAN S. European Union regulations on algorithmic decision-making and a "right to explanation"[J]. AI Magazine, 2017, 38(3):50-57.
8. ZWITTER A. Big data ethics[J]. Big Data & Society, 2014, 1(2):2053951714559253.
9. LUNDBERG S M, LEE S I. A unified approach to interpreting model predictions[C]//Advances in Neural Information Processing Systems. 2017:4765-4774.
10. RIBEIRO M T, SINGH S, GUESTRIN C. "Why should I trust you?": Explaining the predictions of any classifier[C]//Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016:1135-1144.
11. REISMAN D, SCHULTZ J, CRAWFORD K, WHITTAKER M. Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability[R]. AI Now Institute, 2018.
12. 