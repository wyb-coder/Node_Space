## 花卉分类比赛：落地训练与提交方案 v2（整合 + 细化 + 不均衡/缩放/早停补充）



---

## 1. 赛题与边界条件（与 1.md 一致）

- 训练集：`data/train/`；标注文件 `data/train_labels.csv`（至少包含：`filename, category_id`；可附带 `chinese_name, english_name`，训练时忽略）。
- 评测入口：评测方运行 `code/predict.py`，对 `test_dataset/*.jpg` 推理，输出 `results/results/submission.csv`。提交列与顺序以评测方要求为准；本仓库参考样例为 `ref/predictions_sample.csv`（`filename,category_id,confidence`），若仅需两列请导出 `filename,category_id`。
- 允许：预训练权重、数据增强、TTA、融合；不允许外部数据。
- 评测关注：Clean Top-1 与鲁棒性（对常见扰动的稳定性）。

- 补充：
(1)训练过程及最终模型保存到results/model下
(2)假设所需要的预训练模型已经保存至results/model/pretrained_model/下
所需要的预训练模型分别为：
convnextv2_base.fcmae_ft_in22k_in1k
vit_base_patch16_224.augreg2_in21k_ft_in1k

---

## 2. 数据统计、划分与复现

- 类别统计：从 `train_labels.csv` 计算类别数 C、每类样本数 n_c，记录长尾程度（建议输出到 CSV/日志）。
- 划分策略：默认 5 折 Stratified K-Fold（按 `category_id` 分层）。
- 标签映射：构建 `category_id -> class_idx(0..C-1)` 与其逆映射；训练/评估用 `class_idx`；提交时用原始 `category_id` 恢复。
- 近重复样本防泄漏【落实】：可选 pHash 或特征聚类先分组，再按组做分层 K 折（资源有限时可跳过）。
- 复现性【落实】：
	- 统一随机种子（Python/NumPy/PyTorch）；
	- `cudnn.deterministic=True`，`cudnn.benchmark=False`；
	- 固化划分索引、类别映射与配置快照，训练日志与权重带时间戳/折号/种子；
	- 建议将上述元信息保存至 `results/results/exp_name/meta/`。

---

## 3. 预处理与数据增强（含渐进分辨率与鲁棒增强）

- 输入尺寸（渐进式）【落实】：前半程 224；后半程升至 320/384；若显存允许，可 448 精修 5–10 epoch。
- 训练增强（默认以 320 为例）：
	- RandomResizedCrop(320, scale=0.8~1.0)
	- RandomHorizontalFlip(p=0.5)
	- RandomVerticalFlip(p=0.1)、RandomRotation(±10°)
	- ColorJitter(0.2, 0.2, 0.2, 0.1)
	- ToTensor + Normalize(ImageNet 均值方差)
	- Random Erasing(p=0.1)【落实】
	- Mixup(α=0.2) + CutMix(α=0.8)，总强度≈0.4【落实】
	- 可选低强度 AugMix 或 ImageNet-C 风格扰动的少量采样【落实】
- 验证/测试：Resize(短边≈1.14×输入) + CenterCrop(输入尺寸) + Normalize。
- TTA【落实】：多尺度（{320, 384}）× 水平翻转；logit 均值；色彩不作强增强以避免分布偏移。

---

## 4. 模型与分类头

- 主干首选：ConvNeXtV2-Base（`convnextv2_base.fcmae_ft_in22k_in1k`）或 EfficientNetV2-M；
- 第二骨干（融合增益）：ViT-B/16（EVA/CLIP 权重）或 Swin；
- 分类头：默认 Linear；可选 ArcFace/CosFace（margin 0.2~0.4）对比消融；
- 位置编码与归一化需与预训练对齐（ViT 注意插值策略）。

---

## 5. 类别不均衡处理（新增补充：采样策略 + 损失层配套）

以当前 CSV 的真实分布为准（C 为 `unique(category_id)` 数量）。少数类仍可能受影响，建议“数据层 + 损失层”二选一或组合，避免过度矫正。

### 5.1 数据层采样策略

1) WeightedRandomSampler（按类频权重抽样）
	- 简单权重：$w_c = \frac{1}{n_c}$；样本权重为其类别权重；
	- 有效样本数（Class-Balanced by Effective Number）【推荐】：
		$$E(n_c) = \frac{1 - \beta^{n_c}}{1 - \beta},\quad w_c \propto \frac{1}{E(n_c)},\ \beta\in[0.9,0.999]$$
		默认取 $\beta=0.999$，对中等长尾更稳。

2) Class-Balanced Sampler（每个 mini-batch 尽量各类均衡）
	- 以“类”为单位轮转抽样，或每 epoch 保证各类采样次数近似相同；
	- 对样本少的类别执行“过采样”，对样本多的类别允许轻微“欠采样”；
	- 防止过拟合：设置过采样上限（例如不超过原样本数 3×）；并与 Mixup/CutMix 协同使用以缓解重复带来的过拟合。

3) Repeat Factor/阈值过采样（轻量）
	- 给定阈值 t 与指数 $\alpha$，令：$R_c = \max\big(1, (\tfrac{t}{n_c})^{\alpha}\big)$，将少数类重复若干次（取整）；
	- 典型：$t=\operatorname{median}(n_c)$，$\alpha\in[0.3,0.7]$。

4) 欠采样（不建议默认开启）
	- 若总步数过大或训练预算紧张，可对最大类做轻微欠采样（≤20%），与上面 1) 或 2) 组合。

实现建议：
- 首选方案 A：Effective-Number WeightedRandomSampler + LSCE（见 6 章）—简单稳健；
- 方案 B：Class-Balanced Sampler + 标准 CE/LSCE + Mixup/CutMix；
- 不建议同时用 WeightedRandomSampler 与 Balanced Softmax（两者都在纠偏先验，易过补）。

### 5.2 损失层策略

- Label Smoothing Cross Entropy（LSCE，默认 $\epsilon=0.05$）
- Focal Loss（$\gamma\in[1,2]$，可用于难类提升，但与强增强同时用需谨慎）
- Balanced Softmax Cross Entropy（BSCE）【类先验修正】：
	$$p(y=c\mid x)=\frac{n_c\,e^{z_c}}{\sum_j n_j\,e^{z_j}},\ \ \mathcal{L}=-\log p(y=c\mid x)$$
	其中 $z_c$ 为类别 c 的 logit，$n_c$ 为该类训练样本数。BSCE 与均衡采样不建议叠加。

评估与告警：同时监控 Macro-Averaged Top-1/F1，以发现对少数类的退化。

---

## 6. 优化器、学习率与批量自适应缩放（新增补充：统一公式）

- 优化器：AdamW（默认）；SGD 可作为对照；
- 权重衰减：0.05（ViT/Swin 可 0.05~0.1）；
- 学习率基准与线性缩放【落实】：
	- 参考学习率（单卡 256 全局等效 batch）：$\text{lr}_{\text{ref}}=2\times10^{-4}$；
	- 定义全局 batch：
		$$B_{\text{global}} = B_{\text{per\_GPU}}\times \text{world\_size} \times \text{grad\_accum}$$
	- 线性缩放规则：
		$$\text{lr} = \text{lr}_{\text{ref}}\times \frac{B_{\text{global}}}{256}$$
	- 单卡/多卡/累积统一：调节 `B_per_GPU`、`world_size`（GPU 数）、`grad_accum` 即可自动得到新 lr。
	- 上下限建议：$\text{lr}\in[5\times10^{-6}, 5\times10^{-4}]$；超大 batch 可适度低于线性比例（例如 ×0.8）。
- 热身与调度【落实】：Warmup 5 epoch（线性升至目标 lr）+ Cosine Decay；
- 分层学习率（LLRD）【落实】（示例，ConvNeXt/ViT 通用）：
	- 前层×0.25，中层×0.5，后层×1.0，分类头×1.5；
	- ViT 可采用“逐步解冻”：阶段性放开更多 block 的训练。
- AMP 混合精度 + 梯度裁剪（clip_norm=1.0）。

---

## 7. 训练流程（单折）与默认建议

- 轮数：40 epoch（224→320/384 渐进）；若升至 448，再精修 5–10 epoch；
- 批大小：按显存确定；以 320 为例，每卡 64 左右（AMP + 累积可放大等效 batch）；
- 学习率：按第 6 章线性缩放公式计算；
- 正则：LSCE(ε=0.05)、Mixup=0.2、CutMix=0.8、Random Erasing=0.1；
- EMA【落实】：衰减 0.9999，验证与保存均评估“原模型与 EMA 模型”并择优；
- SWA/快照集成（可选）；
- 多卡 DDP 与 SyncBN（CNN）/LN（ViT）。
- 训练过程可视化输出（步骤，训练到了哪个epoch、进度是多少、现在训练集Loss、验证集Loss是多少）等等输出。

### 7.1 早停与模型选择（新增补充）

- 监控指标：
	- Clean 验证集 Top-1（`val_acc_clean`）；
	- 可选“腐蚀验证集”Top-1（`val_acc_corr`）；
	- 综合评分（默认作为 best 选择依据）：
		$$S = 0.7\cdot \text{val\_acc\_clean} + 0.3\cdot \text{val\_acc\_corr}$$
- 早停策略：
	- `patience=10`（epoch 级），`min_delta=0.001`；
	- 监控指标：默认用综合评分 S；若未启用腐蚀集则用 `val_acc_clean`；
	- 满足“连续 `patience` 个 epoch 无显著提升（< min_delta）”则提前停止该折训练；
	- Tie-breaker：若分数相等，优先选择更早出现的权重；
	- EMA 优先：每次评估对“原模型与 EMA”都计算指标，保存分数更高的一方为 best。若 EMA 失效（罕见），回退到原模型 best。
- Checkpoint 规范：
	- `best_clean.pth`、`best_combo.pth`（按 S 保存）、`ema_best.pth`、`last.pth`；
	- 文件命名含 `fold{K}_seed{S}_epoch{E}_score{S}.pth`，便于自动发现与融合。

### 7.2 渐进分辨率节奏（示例）

- Epoch 1–15：224，Mixup/CutMix 正常；
- Epoch 16–40：320 或 384；
- （可选）Epoch 41–48：448，关闭或减弱 Mixup/CutMix，仅做短程精修；
- 升维时将学习率按 Cosine 连续衔接；若显存紧张，可减小 batch 并依线性缩放公式微调 lr。

---

## 8. 验证评估与鲁棒性（与 1.md 对齐并细化）

- 指标：Clean Top-1、Corruption Top-1（若启用）、Macro Top-1/F1；可选校准指标 ECE；
- 腐蚀验证集构建【落实】：
	- 对验证集生成若干扰动（高斯噪声、模糊、亮度、JPEG 压缩、轻遮挡等），强度低–中；
	- 逐类统计与总体均值，作为增强与模型选择参考；
- 误差分析【落实】：
	- 混淆矩阵、每类精召与 Top-k 错误样本清单；
	- Grad-CAM 检查注意力是否集中在花朵区域，指导裁剪与增强调参。

---

## 9. 推理、TTA、融合与温度缩放（与 1.md 对齐并细化）

- 预测入口：`code/predict.py`；预处理与训练对齐；
- TTA【落实】：多尺度（320、384）× 水平翻转；logit 求均值；
- 多折/多种子/多骨干融合【落实】：发现 `results/model/*.pth`（或按命名模式），加载逐个前向、logit 平均；
- 温度缩放（Temperature Scaling）【落实】：
	- 在每折验证集拟合温度 T（最小化 NLL），推理时对融合后的 logit 除以 T；
	- 先在 Clean 上拟合 T；若鲁棒集权重较高，可对综合验证集拟合；
- 

---

## 10. 工程化与可复现（与 1.md 对齐并补充配置键）

- 配置化【落实】：将关键超参集中到配置（argparse/YAML），建议的配置键：
	- 数据：`data.root`, `data.csv`, `data.image_col`, `data.label_col`, `data.num_classes`, `data.fold`, `data.n_splits`, `data.img_size`
	- 采样：`sampler.type in {none, weighted_en, class_balanced}`, `sampler.beta`, `sampler.oversample_cap`
	- 模型：`model.backbone`, `model.pretrained`, `model.head in {linear, arcface, cosface}`, `model.margin`
	- 优化：`opt.name`, `opt.lr_ref`, `opt.weight_decay`, `opt.momentum`, `opt.clip_grad`
	- 批量与缩放：`train.batch_per_gpu`, `train.world_size`, `train.grad_accum`（据此计算 `B_global` 与 lr）
	- 调度：`sched.warmup_epochs`, `sched.cosine_epochs`
	- LLRD/解冻：`llrd.enabled`, `llrd.multipliers`, `llrd.unfreeze_schedule`
	- 正则：`reg.ls_eps`, `reg.mixup`, `reg.cutmix`, `reg.erase_p`
	- EMA/SWA：`ema.enabled`, `ema.decay`, `swa.enabled`
	- 验证与鲁棒：`robust.enabled`, `robust.kinds`, `robust.level`
	- 早停与选择：`es.enabled`, `es.patience`, `es.min_delta`, `select.metric in {clean, combo}`, `select.alpha`（combo 权重）
	- TTA/融合/温度：`tta.scales`, `tta.hflip`, `ensembling.find_pattern`, `ts.enabled`
	- 复现：`seed`, `cudnn.deterministic`, `cudnn.benchmark`
- 日志追踪【落实】：保存训练曲线、关键指标、样例图（增强前后对比）、最佳权重路径；
- 权重保存与回载【落实】：保存 best（含 EMA）；推理脚本可按命名规则自动发现并融合；
- 归一化与权重一致性：严格对齐预训练权重所用均值方差与输入尺寸。

---

## 11. 默认关键超参（可直接使用）

- 模型：ConvNeXtV2-Base（主） + ViT-B/16（辅融合）
- 输入尺寸：224（前 15 epoch）→ 320/384（后 25 epoch）→（可选）448（5–10 epoch）
- 优化器：AdamW；`lr_ref=2e-4`；`weight_decay=0.05`；
- 线性缩放：$\text{lr} = 2\times10^{-4} \times B_{\text{global}}/256$；
- LLRD：前×0.25，中×0.5，后×1.0，头×1.5；
- 正则：LSCE(ε=0.05)，Mixup=0.2，CutMix=0.8，RandomErasing=0.1；
- AMP & Clip：AMP=True，`clip_norm=1.0`；
- EMA：0.9999；SWA：False（可按需开启）；
- 采样：`sampler=weighted_en`，`beta=0.999`（或 `class_balanced`）；过采样上限 3×；
- 早停：`patience=10`，`min_delta=0.001`，`metric=combo`，`alpha=0.7`（Clean:Corr=0.7:0.3）。

---

## 12. 验收与提交清单（与 1.md 对齐）

- Clean Acc 与 Corruption Acc 达到预设阈值，Macro 指标无异常回退；
- 多折/多种子/异构融合稳定提升；
- 早停记录完备，节省算力且不损失分数；
- 提交路径与格式严格符合：`results/results/submission.csv`；
- 附上配置/权重/日志与复现说明；推理脚本支持自动融合与温度缩放。

---

## 13. 风险与应对（补充不均衡/缩放/早停）

- 过拟合与信息泄漏：严格分层 K 折（可选分组）、增强强度适中、检查评估与 TTA 一致性；
- 少数类退化：启用 Effective-Number WeightedSampler 或 Class-Balanced Sampler；监控 Macro 指标；必要时尝试 BSCE/Focal；
- 学习率不匹配：按线性缩放公式统一；Warmup 5 epoch；如大 batch 不稳，降低比例或增大衰减；
- 训练不稳定：Warmup、LLRD、梯度裁剪、EMA；必要时降低增强强度或学习率；
- 资源与时间：两张 4090 并行不同折/骨干；早停（`patience=10`）显著节省算力；优先稳定基线再做增益实验。

---

### 附：最小可用配置样例（YAML 片段，便于迁移到 argparse）

```yaml
seed: 42
data:
	root: data/train
	csv: data/train_labels.csv
	# 列名适配：
	image_col: filename
	label_col: category_id
	# 类别数自动从 CSV 推断（unique(label_col)）；如需强制指定可设置：
	num_classes: auto
	n_splits: 5
	img_size: [224, 320]   # 渐进分辨率序列
sampler:
	type: weighted_en       # {none, weighted_en, class_balanced}
	beta: 0.999
	oversample_cap: 3
model:
	backbone: convnextv2_base.fcmae_ft_in22k_in1k
	head: linear            # {linear, arcface, cosface}
	margin: 0.3
opt:
	name: adamw
	lr_ref: 2.0e-4
	weight_decay: 0.05
train:
	batch_per_gpu: 64
	world_size: 2
	grad_accum: 1
	amp: true
	clip_grad: 1.0
sched:
	warmup_epochs: 5
	cosine_epochs: 40
llrd:
	enabled: true
	multipliers: [0.25, 0.5, 1.0, 1.5]
reg:
	ls_eps: 0.05
	mixup: 0.2
	cutmix: 0.8
	erase_p: 0.1
ema:
	enabled: true
	decay: 0.9999
robust:
	enabled: true
	kinds: [gaussian_noise, blur, brightness, jpeg, occlusion]
	level: low
es:
	enabled: true
	patience: 10
	min_delta: 0.001
select:
	metric: combo
	alpha: 0.7
tta:
	scales: [320, 384]
	hflip: true
ts:
	enabled: true
```

> 说明：按照上面的键即可在训练脚本中一一对照实现；学习率请在初始化时依公式自动从 `lr_ref` 与 `B_global` 推导得到，以避免迁移时漏调。

